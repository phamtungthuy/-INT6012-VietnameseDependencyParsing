{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvoz16XuV19I",
        "outputId": "90327728-a8d9-49c2-fc6b-ee632e4d385c",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'UD_Vietnamese-VTB'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 802, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 802 (delta 32), reused 27 (delta 23), pack-reused 763 (from 3)\u001b[K\n",
            "Receiving objects: 100% (802/802), 20.76 MiB | 14.19 MiB/s, done.\n",
            "Resolving deltas: 100% (431/431), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Vietnamese-VTB.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm1BjtUQWOT-",
        "outputId": "69546236-dfca-4546-8e83-bd7268680536",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2026.1.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.17.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.4.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.3.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install conllu\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install gensim\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2026.1.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l_MgUFhiXygf",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from conllu import parse_incr\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Đã thiết lập random seed: 42\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # Nếu dùng multi-GPU\n",
        "    \n",
        "    # Đảm bảo tính toán trên cuDNN là deterministic (có thể làm chậm tốc độ train một chút)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "    print(f\"✅ Đã thiết lập random seed: {seed}\")\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def map_sentence_to_ids(sentences, p_vocab, d_vocab):\n",
        "    \"\"\"Chuyển đổi toàn bộ nhãn UPOS và DEPREL trong câu sang ID trước khi training.\"\"\"\n",
        "    for sent in sentences:\n",
        "        for token in sent:\n",
        "            token['upos_id'] = p_vocab.get(token['upos'], p_vocab.get(\"<UNK>\", 0))\n",
        "            token['deprel_id'] = d_vocab.get(token['deprel'], 0)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EaHba2LQhpjx",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "class Oracle:\n",
        "    def __init__(self, vocab_pos, vocab_deps):\n",
        "        self.p_vocab = vocab_pos\n",
        "        self.d_vocab = vocab_deps\n",
        "        # Danh sách ID các nhãn quan hệ để giả lập việc \"gán nhầm\"\n",
        "        self.all_dep_ids = [v for k, v in vocab_deps.items() if k not in [\"<NULL>\", \"<ROOT>\", \"<UNK>\"]]\n",
        "        self.action_map = self._build_action_map()\n",
        "\n",
        "    def _build_action_map(self):\n",
        "        mapping = {\"SHIFT\": 0}\n",
        "        idx = 1\n",
        "        for dep in self.d_vocab:\n",
        "            if dep not in [\"<NULL>\", \"<ROOT>\", \"<UNK>\"]:\n",
        "                mapping[f\"LEFT-ARC_{dep}\"] = idx\n",
        "                idx += 1\n",
        "                mapping[f\"RIGHT-ARC_{dep}\"] = idx\n",
        "                idx += 1\n",
        "        return mapping\n",
        "\n",
        "    def extract_features(self, stack, buffer, sentence, sent_idx, left_children_map, right_children_map, \n",
        "                         stack_size=3, buffer_size=3, mutate=False):\n",
        "        # 1. STACK VIEW\n",
        "        s_window = stack[-stack_size:] if stack else [0]\n",
        "        if len(s_window) < stack_size:\n",
        "            # Pad LEFT để các phần tử quan trọng (s0, s1) nằm ở cuối list giống như stack thật\n",
        "            s_window = [None] * (stack_size - len(s_window)) + s_window \n",
        "            \n",
        "        s_feats = {\n",
        "            'words': [], 'pos': [],\n",
        "            'l_child_idx': [], 'l_child_dep': [],  'l_child_pos': [],\n",
        "            'r_child_idx': [], 'r_child_dep': [],  'r_child_pos': [] \n",
        "        }\n",
        "\n",
        "        for i, idx in enumerate(s_window):\n",
        "            if idx is None:\n",
        "                for k in s_feats: s_feats[k].append(0)\n",
        "                continue\n",
        "\n",
        "            s_feats['words'].append(idx)\n",
        "            if idx == 0: # ROOT\n",
        "                s_feats['pos'].append(0)\n",
        "                for k in ['l_child_idx', 'l_child_dep', 'l_child_pos', 'r_child_idx', 'r_child_dep', 'r_child_pos']:\n",
        "                    s_feats[k].append(0)\n",
        "            else:\n",
        "                token = sentence[idx - 1]\n",
        "                s_feats['pos'].append(token['upos_id'])\n",
        "\n",
        "                l_children = left_children_map.get(idx, [])\n",
        "                r_children = right_children_map.get(idx, [])\n",
        "\n",
        "                # Lấy thông tin con thực tế (Gold)\n",
        "                l_idx, l_dep, l_pos = l_children[0] if l_children else (0, 0, 0)\n",
        "                r_idx, r_dep, r_pos = r_children[-1] if r_children else (0, 0, 0)\n",
        "\n",
        "                # --- LOGIC MUTATION (GIẢ LẬP SAI BƯỚC TRƯỚC) ---\n",
        "                # i < stack_size - 1 đảm bảo chỉ mutate s1, s2... (các từ đã nằm lâu trên stack)\n",
        "                if mutate and i < (stack_size - 1):\n",
        "                    # Giả lập việc gán nhầm nhãn quan hệ cho các con của s1, s2\n",
        "                    if l_dep != 0 and random.random() < 0.5:\n",
        "                        l_dep = random.choice(self.all_dep_ids)\n",
        "                    if r_dep != 0 and random.random() < 0.5:\n",
        "                        r_dep = random.choice(self.all_dep_ids)\n",
        "                # -----------------------------------------------\n",
        "\n",
        "                s_feats['l_child_idx'].append(l_idx)\n",
        "                s_feats['l_child_dep'].append(l_dep)\n",
        "                s_feats['l_child_pos'].append(l_pos)\n",
        "                s_feats['r_child_idx'].append(r_idx)\n",
        "                s_feats['r_child_dep'].append(r_dep)\n",
        "                s_feats['r_child_pos'].append(r_pos)\n",
        "\n",
        "        # 2. BUFFER VIEW\n",
        "        b_window = buffer[:buffer_size] if buffer else [0]\n",
        "        if len(b_window) < buffer_size:\n",
        "            b_window = b_window + [None] * (buffer_size - len(b_window))\n",
        "            \n",
        "        b_feats = {'words': [], 'pos': []}\n",
        "        for idx in b_window:\n",
        "            if idx is None:\n",
        "                b_feats['words'].append(0); b_feats['pos'].append(0)\n",
        "                continue\n",
        "            b_feats['words'].append(idx)\n",
        "            b_feats['pos'].append(0 if idx == 0 else sentence[idx-1]['upos_id'])\n",
        "\n",
        "        return {'stack': s_feats, 'buffer': b_feats, 'sent_id': sent_idx}\n",
        "\n",
        "    def create_training_data_multiview(self, sentence, sent_idx, aug_p=0.2):\n",
        "        \"\"\"\n",
        "        aug_p: Xác suất tạo thêm một mẫu dữ liệu nhiễu tại mỗi bước transition.\n",
        "        \"\"\"\n",
        "        stack, buffer = [0], list(range(1, len(sentence) + 1))\n",
        "        left_children, right_children = {}, {}\n",
        "        train_examples = []\n",
        "        gold_heads = {token['id']: token['head'] for token in sentence}\n",
        "\n",
        "        while buffer or len(stack) > 1:\n",
        "            # 1. Trích xuất đặc trưng GOLD (Dữ liệu sạch)\n",
        "            feat_gold = self.extract_features(stack, buffer, sentence, sent_idx, left_children, right_children, mutate=False)\n",
        "\n",
        "            # --- Xác định Action ID đúng ---\n",
        "            target_action_id = None\n",
        "            if len(stack) >= 2:\n",
        "                s1, s2 = stack[-1], stack[-2]\n",
        "                if gold_heads.get(s2) == s1: # Left-Arc\n",
        "                    token_s2 = sentence[s2 - 1]\n",
        "                    target_action_id = self.action_map.get(f\"LEFT-ARC_{token_s2['deprel']}\", 0)\n",
        "                elif gold_heads.get(s1) == s2 and not any(gold_heads.get(b) == s1 for b in buffer): # Right-Arc\n",
        "                    token_s1 = sentence[s1 - 1]\n",
        "                    target_action_id = self.action_map.get(f\"RIGHT-ARC_{token_s1['deprel']}\", 0)\n",
        "                elif buffer: # Shift\n",
        "                    target_action_id = self.action_map[\"SHIFT\"]\n",
        "            elif buffer:\n",
        "                target_action_id = self.action_map[\"SHIFT\"]\n",
        "\n",
        "            if target_action_id is None: break\n",
        "\n",
        "            # 2. THÊM DỮ LIỆU VÀO TẬP HUẤN LUYỆN\n",
        "            # Thêm mẫu Gold (luôn giữ nguyên dữ liệu gốc)\n",
        "            train_examples.append((feat_gold, target_action_id))\n",
        "\n",
        "            # Augmentation: Thêm mẫu bị mutation (giả lập sai bước trước)\n",
        "            if aug_p > 0 and random.random() < aug_p:\n",
        "                feat_mutated = self.extract_features(stack, buffer, sentence, sent_idx, \n",
        "                                                     left_children, right_children, mutate=True)\n",
        "                # Dạy mô hình: \"Dù đặc trưng này bị nhiễu (mutation), hành động đúng vẫn là target_action_id\"\n",
        "                train_examples.append((feat_mutated, target_action_id))\n",
        "\n",
        "            # 3. CẬP NHẬT TRẠNG THÁI (Sử dụng Gold Action để đi tiếp - Teacher Forcing)\n",
        "            if target_action_id == self.action_map[\"SHIFT\"]:\n",
        "                stack.append(buffer.pop(0))\n",
        "            else:\n",
        "                action_name = [k for k, v in self.action_map.items() if v == target_action_id][0]\n",
        "                if action_name.startswith(\"LEFT-ARC\"):\n",
        "                    child = stack.pop(-2)\n",
        "                    head = stack[-1]\n",
        "                    if head not in left_children: left_children[head] = []\n",
        "                    left_children[head].append((child, sentence[child-1]['deprel_id'], sentence[child-1]['upos_id']))\n",
        "                else: # RIGHT-ARC\n",
        "                    child = stack.pop(-1)\n",
        "                    head = stack[-1]\n",
        "                    if head not in right_children: right_children[head] = []\n",
        "                    right_children[head].append((child, sentence[child-1]['deprel_id'], sentence[child-1]['upos_id']))\n",
        "\n",
        "        return train_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vh4vxdfYmaZq",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def load_data_and_build_vocab(file_path, is_train=True, word_v=None, pos_v=None, dep_v=None):\n",
        "    sentences = []\n",
        "    word_counts = Counter()\n",
        "    pos_counts = Counter()\n",
        "    dep_counts = Counter()\n",
        "\n",
        "    for path in file_path:\n",
        "\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "          for tokenlist in parse_incr(f):\n",
        "              # Chỉ thêm những câu có nội dung\n",
        "              if len(tokenlist) > 0:\n",
        "                  if not is_train or 'train' in path:\n",
        "                      sentences.append(tokenlist)\n",
        "                  if is_train:\n",
        "                      for token in tokenlist:\n",
        "                          word_counts[token['form'].lower()] += 1\n",
        "                          pos_counts[token['upos']] += 1\n",
        "                          dep_counts[token['deprel']] += 1\n",
        "\n",
        "    if is_train:\n",
        "        # Khởi tạo ID theo yêu cầu: NULL=0, ROOT=1, UNK=2\n",
        "        def create_mapping(counts, is_dep=False):\n",
        "            vocab = {\"<NULL>\": 0, \"<ROOT>\": 1}\n",
        "            if not is_dep: vocab[\"<UNK>\"] = 2\n",
        "            for item, count in counts.items():\n",
        "                if item not in vocab and (is_dep or count > 1):\n",
        "                    vocab[item] = len(vocab)\n",
        "            return vocab\n",
        "\n",
        "        word_v = create_mapping(word_counts)\n",
        "        pos_v = create_mapping(pos_counts)\n",
        "        dep_v = create_mapping(dep_counts, is_dep=True)\n",
        "        return sentences, word_v, pos_v, dep_v\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bJkcAq-QbKdO",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_path, dev_path, test_path):\n",
        "    # BƯỚC 1: Load dữ liệu và chỉ xây dựng Vocab cho POS và Dep\n",
        "    # Bạn có thể dùng lại hàm load_data_and_build_vocab cũ nhưng bỏ qua phần word_v\n",
        "    print(\"Đang đọc tập Train và xây dựng Vocab cho POS/Dep...\")\n",
        "    train_sentences, _, p_vocab, d_vocab = load_data_and_build_vocab([train_path], is_train=True)\n",
        "\n",
        "    # BƯỚC 2: Khởi tạo Oracle mới (Không dùng w_vocab)\n",
        "    # Đảm bảo bạn đã cập nhật lớp Oracle như hướng dẫn ở bước trước\n",
        "    oracle = Oracle(p_vocab, d_vocab)\n",
        "\n",
        "    # BƯỚC 3: Tạo dữ liệu huấn luyện (X, y) dạng Sequence\n",
        "    all_training_data = []\n",
        "    print(f\"Đang tạo chuỗi transitions cho {len(train_sentences)} câu tập Train...\")\n",
        "\n",
        "    for idx, sentence in enumerate(train_sentences):\n",
        "        try:\n",
        "            # create_training_data bây giờ trả về danh sách các dict chứa chuỗi indices\n",
        "            examples = oracle.create_training_data(sentence, idx)\n",
        "            all_training_data.extend(examples)\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi logic tại câu: {sentence.metadata['text']} - {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Thành công! Tổng số bước transition huấn luyện: {len(all_training_data)}\")\n",
        "\n",
        "    # BƯỚC 4: Load tập Dev và Test (giữ nguyên câu gốc để PhoBERT encode sau này)\n",
        "    print(\"Đang chuẩn bị tập Dev và Test...\")\n",
        "    dev_sentences = load_data_and_build_vocab([dev_path], is_train=False)\n",
        "    test_sentences = load_data_and_build_vocab([test_path], is_train=False)\n",
        "\n",
        "    print(f\"Số câu: Train={len(train_sentences)}, Dev={len(dev_sentences)}, Test={len(test_sentences)}\")\n",
        "\n",
        "    # Trả về thêm train_sentences để tí nữa dùng lấy PhoBERT embedding\n",
        "    return all_training_data, (p_vocab, d_vocab), (train_sentences, dev_sentences, test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2KxUZ56XyDKr",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TransitionDataset(Dataset):\n",
        "    def __init__(self, training_examples):\n",
        "        self.examples = training_examples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat, action_id = self.examples[idx]\n",
        "\n",
        "        # Stack features\n",
        "        s_words = torch.tensor(feat['stack']['words'], dtype=torch.long)\n",
        "        s_pos = torch.tensor(feat['stack']['pos'], dtype=torch.long)\n",
        "        s_lc_idx = torch.tensor(feat['stack']['l_child_idx'], dtype=torch.long)\n",
        "        s_lc_dep = torch.tensor(feat['stack']['l_child_dep'], dtype=torch.long) # MỚI\n",
        "        s_lc_pos = torch.tensor(feat['stack']['l_child_pos'], dtype=torch.long) # MỚI\n",
        "        s_rc_idx = torch.tensor(feat['stack']['r_child_idx'], dtype=torch.long)\n",
        "        s_rc_dep = torch.tensor(feat['stack']['r_child_dep'], dtype=torch.long) # MỚI\n",
        "        s_rc_pos = torch.tensor(feat['stack']['r_child_pos'], dtype=torch.long) # MỚI\n",
        "\n",
        "        # Buffer features\n",
        "        b_words = torch.tensor(feat['buffer']['words'], dtype=torch.long)\n",
        "        b_pos = torch.tensor(feat['buffer']['pos'], dtype=torch.long)\n",
        "\n",
        "        return (s_words, s_pos, s_lc_idx, s_lc_dep, s_lc_pos, s_rc_idx, s_rc_dep, s_rc_pos,\n",
        "                b_words, b_pos, torch.tensor(action_id), feat['sent_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def multiview_collate_fn(batch):\n",
        "    # Unpack 11 thành phần\n",
        "    (s_words, s_pos, s_lc_idx, s_lc_dep, s_lc_pos, s_rc_idx, s_rc_dep, s_rc_pos,\n",
        "     b_words, b_pos, labels, sent_ids) = zip(*batch)\n",
        "\n",
        "    # Helper function để pad cho gọn\n",
        "    def pad(tensors): return pad_sequence(tensors, batch_first=True, padding_value=0)\n",
        "\n",
        "    return (\n",
        "        pad(s_words), pad(s_pos),\n",
        "        pad(s_lc_idx), pad(s_lc_dep), pad(s_lc_pos), # Pad LC\n",
        "        pad(s_rc_idx), pad(s_rc_dep), pad(s_rc_pos), # Pad RC\n",
        "        pad(b_words), pad(b_pos),\n",
        "        torch.stack(labels), sent_ids\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SubViewProjector(nn.Module):\n",
        "    \"\"\"Chiếu các nhóm đặc trưng (Word+POS/Dep) về cùng một không gian ẩn.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.LayerNorm(output_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "        \n",
        "class GatedFusion(nn.Module):\n",
        "    \"\"\"Sử dụng cổng Sigmoid để kiểm soát luồng thông tin từ Main và các con.\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        # Input là 3 views nối tầng (Main, LC, RC) -> Output là 3 bộ trọng số tương ứng\n",
        "        self.gate_layer = nn.Sequential(\n",
        "            nn.Linear(dim * 3, dim * 3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.project = nn.Linear(dim * 3, dim)\n",
        "        self.layer_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, views):\n",
        "        # views shape: [batch, 3, dim] (đã được trích xuất cho từng token trên stack)\n",
        "        # Lưu ý: Ở đây xử lý cho từng vị trí trên cửa sổ stack (S0, S1, S2)\n",
        "        batch, n_tokens, n_views, dim = views.shape\n",
        "        \n",
        "        # Flatten views: [batch, n_tokens, dim * 3]\n",
        "        flat_views = views.view(batch, n_tokens, -1)\n",
        "        \n",
        "        # Tính toán bộ lọc (gate)\n",
        "        gates = self.gate_layer(flat_views) # [batch, n_tokens, dim * 3]\n",
        "        \n",
        "        # Áp dụng lọc thông tin\n",
        "        gated_info = flat_views * gates\n",
        "        \n",
        "        # Chiếu về lại không gian ẩn node_dim\n",
        "        fused = self.project(gated_info)\n",
        "        return self.layer_norm(fused)\n",
        "\n",
        "class BiaffineRelationalProjector(nn.Module):\n",
        "    \"\"\"Sử dụng Biaffine để trích xuất đặc trưng quan hệ Cha-Con thay cho MLP.\"\"\"\n",
        "    def __init__(self, head_dim, child_dim, output_dim, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        # Ma trận Biaffine để học tương tác (Bilinear)\n",
        "        # output_dim ở đây đóng vai trò như các 'feature maps' của quan hệ\n",
        "        self.weight = nn.Parameter(torch.Tensor(output_dim, head_dim + 1, child_dim + 1))\n",
        "        \n",
        "        self.norm = nn.LayerNorm(output_dim)\n",
        "        self.mish = nn.Mish()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, head_embed, child_embed):\n",
        "        # 1. Thêm bias cho cả head và child\n",
        "        # head: [batch, dim] -> [batch, dim+1]\n",
        "        h = torch.cat([head_embed, head_embed.new_ones(head_embed.shape[:-1]).unsqueeze(-1)], -1)\n",
        "        c = torch.cat([child_embed, child_embed.new_ones(child_embed.shape[:-1]).unsqueeze(-1)], -1)\n",
        "        \n",
        "        # 2. Tính tương tác Biaffine: [batch, output_dim]\n",
        "        # Mỗi chiều của output_dim sẽ học một loại 'khớp nối' khác nhau giữa cha và con\n",
        "        rel_feat = torch.einsum('bi,oij,bj->bo', h, self.weight, c)\n",
        "        \n",
        "        return self.dropout(self.mish(self.norm(rel_feat)))\n",
        "\n",
        "class BiaffineDependencyModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pos_vocab_size, dep_vocab_size, num_actions,\n",
        "                 phobert_dim=768, pos_dim=64, dep_dim=64,\n",
        "                 node_dim=256, hidden_dim=1024, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_dim = node_dim\n",
        "\n",
        "        self.pos_embed = nn.Embedding(pos_vocab_size, pos_dim, padding_idx=0)\n",
        "        self.dep_embed = nn.Embedding(dep_vocab_size, dep_dim, padding_idx=0)\n",
        "\n",
        "        # 1. Projector cho Main Word\n",
        "        self.main_proj = SubViewProjector(phobert_dim + pos_dim, node_dim * 2, node_dim)\n",
        "        \n",
        "        # 2. Relational Projectors cho Children (Dùng thông tin cha để hiểu con)\n",
        "        # Input dim = node_dim (của cha) + (phobert + dep + pos của con)\n",
        "        child_input_dim = phobert_dim + dep_dim + pos_dim\n",
        "        self.lc_rel_proj = BiaffineRelationalProjector(node_dim, child_input_dim, node_dim)\n",
        "        self.rc_rel_proj = BiaffineRelationalProjector(node_dim, child_input_dim, node_dim)\n",
        "\n",
        "        self.fusion = GatedFusion(node_dim)\n",
        "        self.buffer_proj = SubViewProjector(phobert_dim + pos_dim, node_dim * 2, node_dim)\n",
        "\n",
        "        combined_dim = 4 * node_dim\n",
        "\n",
        "        # 3. Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(combined_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Mish(), # Dùng Mish cho mượt hơn ReLU\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(hidden_dim // 2, num_actions)\n",
        "        )\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, s_vecs, s_pos, s_lc_vecs, s_lc_dep, s_lc_pos, s_rc_vecs, s_rc_dep, s_rc_pos, b_vecs, b_pos):\n",
        "        # --- BƯỚC 1: Chiếu nút gốc (Main Head) ---\n",
        "        v_main_s = self.main_proj(torch.cat([s_vecs, self.pos_embed(s_pos)], dim=-1))\n",
        "\n",
        "        # --- BƯỚC 2: Chiếu Relational cho nút con ---\n",
        "        # Chúng ta dùng v_main_s như ngữ cảnh để \"nhìn\" các nút con\n",
        "        feat_lc = torch.cat([s_lc_vecs, self.dep_embed(s_lc_dep), self.pos_embed(s_lc_pos)], dim=-1)\n",
        "        feat_rc = torch.cat([s_rc_vecs, self.dep_embed(s_rc_dep), self.pos_embed(s_rc_pos)], dim=-1)\n",
        "        \n",
        "        v_lc = self.lc_rel_proj(v_main_s.view(-1, self.node_dim), feat_lc.view(-1, feat_lc.size(-1)))\n",
        "        v_rc = self.rc_rel_proj(v_main_s.view(-1, self.node_dim), feat_rc.view(-1, feat_rc.size(-1)))\n",
        "        \n",
        "        # Đưa về shape cũ: [batch, 3, node_dim]\n",
        "        v_lc = v_lc.view(v_main_s.shape)\n",
        "        v_rc = v_rc.view(v_main_s.shape)\n",
        "\n",
        "        # --- BƯỚC 3: Gộp thông tin ---\n",
        "        stack_views = torch.stack([v_main_s, v_lc, v_rc], dim=2)\n",
        "        s_node_emb = self.fusion(stack_views) + v_main_s \n",
        "        \n",
        "        # Buffer & Feature Selection giữ nguyên logic cũ\n",
        "        b_node_emb = self.buffer_proj(torch.cat([b_vecs, self.pos_embed(b_pos)], dim=-1))\n",
        "\n",
        "        s0 = s_node_emb[:, -1, :] # Top 1 stack\n",
        "        s1 = s_node_emb[:, -2, :] # Top 2 stack\n",
        "\n",
        "        b0 = b_node_emb[:, 0, :]\n",
        "        b1 = b_node_emb[:, 1, :]\n",
        "  \n",
        "        combined = torch.cat([s0, s1, b0, b1], dim=-1)\n",
        "        return self.classifier(combined)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Hàm này sẽ được apply đệ quy cho mọi module con\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Khởi tạo Linear: Xavier phù hợp với Mish/Tanh\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "                \n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            # KHÔNG dùng Xavier cho LayerNorm. Giữ nguyên 1.0 và 0.0\n",
        "            nn.init.constant_(module.weight, 1.0)\n",
        "            nn.init.constant_(module.bias, 0.0)\n",
        "            \n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Khởi tạo Embedding mượt hơn cho tiếng Việt\n",
        "            nn.init.normal_(module.weight, mean=0, std=0.02)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_tfidf_weights(sentences):\n",
        "    \"\"\"\n",
        "    sentences: List các list chứa token dict: [[{'form': 'Học_sinh', ...}, ...], ...]\n",
        "    Trả về: Dictionary {word: weight}\n",
        "    \"\"\"\n",
        "    # Chuyển đổi format từ list of dicts sang list of strings (cách nhau bởi khoảng trắng)\n",
        "    corpus = [\" \".join([token['form'] for token in sent]) for sent in sentences]\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\") # Giữ nguyên các từ có dấu/gạch dưới\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    # Lấy giá trị TF-IDF trung bình của từng từ trên toàn bộ tập dữ liệu làm trọng số\n",
        "    # (Hoặc bạn có thể lấy giá trị Max tùy vào mục đích)\n",
        "    weights = tfidf_matrix.mean(axis=0).A1\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    return dict(zip(words, weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def pre_encode_sentences(sentences, model_name=\"vinai/phobert-base\", device='cuda'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "    phobert = AutoModel.from_pretrained(model_name).to(device)\n",
        "    phobert.eval()\n",
        "\n",
        "    encoded_bank = {}\n",
        "    for idx, sent in enumerate(tqdm(sentences)):\n",
        "        # CHỈNH SỬA TẠI ĐÂY: Chuyển khoảng trắng thành gạch dưới để khớp với vocab PhoBERT\n",
        "        words = [token['form'].replace(' ', '_') for token in sent] \n",
        "\n",
        "        inputs = tokenizer(words,\n",
        "                           is_split_into_words=True,\n",
        "                           return_tensors=\"pt\",\n",
        "                           truncation=True,\n",
        "                           max_length=256).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = phobert(**inputs)\n",
        "            full_embeds = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "        word_level_embeds = []\n",
        "        ptr = 1 \n",
        "        for word in words:\n",
        "            # Tokenizer lúc này sẽ xử lý 'Trả_lời' như một đơn vị\n",
        "            sub_tokens_for_word = tokenizer.tokenize(word)\n",
        "            num_subtokens = len(sub_tokens_for_word)\n",
        "\n",
        "            word_vecs = full_embeds[ptr : ptr + num_subtokens]\n",
        "            # Nếu 'Trả_lời' bị tách thành nhiều mảnh (sub-words), \n",
        "            # Mean Pooling ở đây sẽ gộp chúng lại thành 1 vector duy nhất cho từ đó\n",
        "            word_mean_vec = word_vecs.mean(dim=0) if word_vecs.size(0) > 0 else torch.zeros(768, device=device)\n",
        "            word_level_embeds.append(word_mean_vec)\n",
        "            ptr += num_subtokens\n",
        "\n",
        "        root_vector = full_embeds[0]\n",
        "        final_matrix = torch.stack([root_vector] + word_level_embeds)\n",
        "        encoded_bank[idx] = final_matrix.cpu()\n",
        "\n",
        "    return encoded_bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "def pre_encode_sentences_tfidf(sentences, model_name=\"vinai/phobert-base\", device='cuda'):\n",
        "    # 1. Khởi tạo Model & Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "    phobert = AutoModel.from_pretrained(model_name).to(device)\n",
        "    phobert.eval()\n",
        "\n",
        "    # 2. Tính toán TF-IDF weights trước\n",
        "    print(\"--- Đang tính toán trọng số TF-IDF ---\")\n",
        "    tfidf_dict = compute_tfidf_weights(sentences)\n",
        "\n",
        "    encoded_bank = {}\n",
        "    print(f\"--- Đang trích xuất TF-IDF Weighted Embedding cho {len(sentences)} câu ---\")\n",
        "\n",
        "    for idx, sent in enumerate(tqdm(sentences)):\n",
        "        words = [token['form'] for token in sent]\n",
        "\n",
        "        # Encode câu\n",
        "        inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\",\n",
        "                           truncation=True, max_length=256).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = phobert(**inputs)\n",
        "            full_embeds = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "        word_level_embeds = []\n",
        "        ptr = 1  # Bỏ qua token <s>\n",
        "\n",
        "        for word in words:\n",
        "            sub_tokens = tokenizer.tokenize(word)\n",
        "            num_subtokens = len(sub_tokens)\n",
        "            \n",
        "            # Lấy các vector sub-tokens và tính Mean để ra vector từ gốc\n",
        "            word_vecs = full_embeds[ptr : ptr + num_subtokens]\n",
        "            \n",
        "            if word_vecs.size(0) > 0:\n",
        "                # Mean pooling cho các sub-tokens\n",
        "                mean_vec = word_vecs.mean(dim=0)\n",
        "                \n",
        "                # Áp dụng trọng số TF-IDF (mặc định là 1.0 nếu từ không có trong TF-IDF)\n",
        "                # Lưu ý: .lower() để đồng bộ hóa key\n",
        "                weight = tfidf_dict.get(word.lower(), 1.0)\n",
        "                \n",
        "                # Để tránh trọng số quá nhỏ (gần bằng 0), bạn có thể dùng công thức: (1 + weight)\n",
        "                weighted_vec = mean_vec * weight\n",
        "                word_level_embeds.append(weighted_vec)\n",
        "            else:\n",
        "                word_level_embeds.append(torch.zeros(768, device=device))\n",
        "\n",
        "            ptr += num_subtokens\n",
        "\n",
        "        # Vector ROOT (<s>) - Thường giữ nguyên trọng số 1.0\n",
        "        root_vector = full_embeds[0]\n",
        "\n",
        "        final_matrix = torch.stack([root_vector] + word_level_embeds)\n",
        "        encoded_bank[idx] = final_matrix.cpu()\n",
        "\n",
        "    return encoded_bank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Tạo bản đồ metadata cho hành động để tránh parse chuỗi trong vòng lặp\n",
        "def build_action_metadata(id_to_action, d_vocab):\n",
        "    action_meta = {}\n",
        "    for act_id, name in id_to_action.items():\n",
        "        if name == \"SHIFT\":\n",
        "            action_meta[act_id] = {'type': 'S', 'label': None, 'dep_id': 0}\n",
        "        elif name.startswith(\"LEFT-ARC\"):\n",
        "            label = name.split(\"_\")[-1]\n",
        "            action_meta[act_id] = {'type': 'L', 'label': label, 'dep_id': d_vocab.get(label, 0)}\n",
        "        elif name.startswith(\"RIGHT-ARC\"):\n",
        "            label = name.split(\"_\")[-1]\n",
        "            action_meta[act_id] = {'type': 'R', 'label': label, 'dep_id': d_vocab.get(label, 0)}\n",
        "    return action_meta\n",
        "\n",
        "# Gọi hàm này TRƯỚC khi vào vòng lặp đánh giá (ngoài hàm process_one_sentence)\n",
        "# action_meta = build_action_metadata(id_to_action, oracle.d_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mMhsWm0fye0z",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(model, train_loader, train_encoded_bank, dev_sentences, dev_encoded_bank,\n",
        "          oracle, epochs=20, lr=0.001, weight_decay=1e-4, device='cuda'):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "    # --- THÊM SCHEDULER ---\n",
        "    # Giảm LR đi 0.5 lần nếu UAS không tăng sau 3 epoch\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-5,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05) # Giảm smoothing xuống 0.05 như đã bàn\n",
        "    model.to(device)\n",
        "\n",
        "    best_uas = 0.0\n",
        "    best_model_state = None\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        batch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for (s_words, s_pos, s_lc_idx, s_lc_dep, s_lc_pos, s_rc_idx, s_rc_dep, s_rc_pos,\n",
        "             b_words, b_pos, labels, sent_ids) in batch_bar: \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Helper để fetch vector từ bank\n",
        "            def fetch_vecs(indices, sids):\n",
        "                return torch.stack([\n",
        "                    train_encoded_bank[sid][indices[i]]\n",
        "                    for i, sid in enumerate(sids)\n",
        "                ]).to(device)\n",
        "\n",
        "            s_vecs    = fetch_vecs(s_words, sent_ids)\n",
        "            s_lc_vecs = fetch_vecs(s_lc_idx, sent_ids)\n",
        "            s_rc_vecs = fetch_vecs(s_rc_idx, sent_ids)\n",
        "            b_vecs    = fetch_vecs(b_words, sent_ids)\n",
        "\n",
        "            # Move auxiliary data to device\n",
        "            s_pos, s_lc_dep, s_lc_pos, s_rc_dep, s_rc_pos, b_pos, labels = [\n",
        "                x.to(device) for x in [s_pos, s_lc_dep, s_lc_pos, s_rc_dep, s_rc_pos, b_pos, labels]\n",
        "            ]\n",
        "\n",
        "            # Forward\n",
        "            logits = model(s_vecs, s_pos, s_lc_vecs, s_lc_dep, s_lc_pos, s_rc_vecs, s_rc_dep, s_rc_pos, b_vecs, b_pos)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # --- 4. EVALUATION ---\n",
        "        print(f\"\\n🔍 Đang đánh giá Epoch {epoch+1}...\")\n",
        "        # should_eval_full = (epoch + 1) % 3 == 0 or epoch > 15\n",
        "        # eval_sentences = dev_sentences if should_eval_full else dev_sentences[:200] # Subset 200 câu\n",
        "        uas, las = evaluate_model_batch(\n",
        "            model, oracle, dev_sentences, dev_encoded_bank, device=device\n",
        "        )\n",
        "        print(f\"✨ Kết quả: UAS: {uas*100:.2f}% | LAS: {las*100:.2f}%\")\n",
        "\n",
        "        # --- CẬP NHẬT SCHEDULER DỰA TRÊN UAS ---\n",
        "        scheduler.step(uas)\n",
        "\n",
        "        # --- 5. SAVE MODEL IF IMPROVED ---\n",
        "        if uas > best_uas:\n",
        "            best_uas = uas\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "            uas_str = f\"{uas:.4f}\"\n",
        "            save_path = f\"best_model_bi_linear_scorer_{epoch+1}_{uas_str}.pth\"\n",
        "            torch.save(best_model_state, save_path)\n",
        "            print(f\"✅ New best model saved: {save_path}\")\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"🥇 Hoàn tất. UAS cao nhất: {best_uas*100:.2f}%\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "PI7QllLja_xe",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def process_one_sentence(sentence, sent_idx, model, encoded_bank, oracle, action_meta, device):\n",
        "    model.eval()\n",
        "    stack, buffer, arcs = [0], list(range(1, len(sentence) + 1)), []\n",
        "    left_children, right_children = {}, {}\n",
        "\n",
        "    # Chuyển matrix sang device 1 lần duy nhất\n",
        "    sent_matrix = encoded_bank[sent_idx].to(device)\n",
        "\n",
        "    while buffer or len(stack) > 1:\n",
        "        # 1. Trích xuất đặc trưng (vẫn dùng hàm extract_features đã tối ưu của bạn)\n",
        "        feat = oracle.extract_features(stack, buffer, sentence, sent_idx, left_children, right_children)\n",
        "        s_f, b_f = feat['stack'], feat['buffer']\n",
        "\n",
        "        # 2. CHUẨN BỊ TENSORS (Tối ưu: Chỉ gọi .to(device) 1 lần cho mỗi nhóm)\n",
        "        # Các feature ID thường nhỏ, gom nhóm để chuyển device hiệu quả hơn\n",
        "        s_idx = torch.tensor(s_f['words'], dtype=torch.long, device=device)\n",
        "        slc_idx = torch.tensor(s_f['l_child_idx'], dtype=torch.long, device=device)\n",
        "        src_idx = torch.tensor(s_f['r_child_idx'], dtype=torch.long, device=device)\n",
        "        \n",
        "        # Nhặt vector PhoBERT\n",
        "        s_vecs, s_lc_vecs, s_rc_vecs = sent_matrix[s_idx].unsqueeze(0), sent_matrix[slc_idx].unsqueeze(0), sent_matrix[src_idx].unsqueeze(0)\n",
        "\n",
        "        # Labels phụ\n",
        "        s_pos = torch.tensor(s_f['pos'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        s_lc_dep = torch.tensor(s_f['l_child_dep'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        s_rc_dep = torch.tensor(s_f['r_child_dep'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        s_lc_pos = torch.tensor(s_f['l_child_pos'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        s_rc_pos = torch.tensor(s_f['r_child_pos'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        b_idx = torch.tensor(b_f['words'], dtype=torch.long, device=device)\n",
        "        b_vecs = sent_matrix[b_idx].unsqueeze(0)\n",
        "        b_pos = torch.tensor(b_f['pos'], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        # 3. DỰ ĐOÁN\n",
        "        with torch.no_grad():\n",
        "            logits = model(s_vecs, s_pos, s_lc_vecs, s_lc_dep, s_lc_pos, s_rc_vecs, s_rc_dep, s_rc_pos, b_vecs, b_pos)\n",
        "            # Lấy list ID hành động đã sắp xếp\n",
        "            sorted_act_ids = torch.argsort(logits[0], descending=True).tolist()\n",
        "\n",
        "        # 4. ÁP DỤNG HÀNH ĐỘNG (Dùng action_meta để bỏ qua parse chuỗi)\n",
        "        applied = False\n",
        "        for act_id in sorted_act_ids:\n",
        "            meta = action_meta[act_id]\n",
        "            act_type = meta['type']\n",
        "            \n",
        "            if act_type == 'S' and buffer: # SHIFT\n",
        "                stack.append(buffer.pop(0))\n",
        "                applied = True\n",
        "                break\n",
        "                \n",
        "            elif act_type == 'L' and len(stack) >= 2 and stack[-2] != 0: # LEFT-ARC\n",
        "                head, child = stack[-1], stack[-2]\n",
        "                arcs.append((head, child, meta['label']))\n",
        "                \n",
        "                # Lưu vào children map dùng ID có sẵn trong meta và sentence\n",
        "                if head not in left_children: left_children[head] = []\n",
        "                left_children[head].append((child, meta['dep_id'], sentence[child-1]['upos_id']))\n",
        "                \n",
        "                stack.pop(-2)\n",
        "                applied = True\n",
        "                break\n",
        "                \n",
        "            elif act_type == 'R' and len(stack) >= 2: # RIGHT-ARC\n",
        "                head, child = stack[-2], stack[-1]\n",
        "                arcs.append((head, child, meta['label']))\n",
        "                \n",
        "                # Lưu vào children map\n",
        "                if head not in right_children: right_children[head] = []\n",
        "                right_children[head].append((child, meta['dep_id'], sentence[child-1]['upos_id']))\n",
        "                \n",
        "                stack.pop(-1)\n",
        "                applied = True\n",
        "                break\n",
        "\n",
        "        if not applied: break\n",
        "\n",
        "    # 5. TÍNH UAS/LAS (Giữ nguyên logic chính nhưng viết gọn)\n",
        "    gold_heads = {t['id']: t['head'] for t in sentence}\n",
        "    gold_deps = {t['id']: t['deprel'] for t in sentence}\n",
        "    pred_info = {child: (head, label) for head, child, label in arcs}\n",
        "\n",
        "    correct_uas = sum(1 for c, g_h in gold_heads.items() if c in pred_info and pred_info[c][0] == g_h)\n",
        "    correct_las = sum(1 for c, g_h in gold_heads.items() if c in pred_info and pred_info[c][0] == g_h and pred_info[c][1] == gold_deps[c])\n",
        "\n",
        "    return correct_uas, correct_las, len(gold_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def process_one_sentence_beam(sentence, sent_idx, model, encoded_bank, oracle, id_to_action, device, beam_size=3):\n",
        "    model.eval()\n",
        "    \n",
        "    # Ma trận PhoBERT cho câu (Lookup table)\n",
        "    sent_matrix = encoded_bank[sent_idx].to(device)\n",
        "    \n",
        "    # Một trạng thái bao gồm: (stack, buffer, arcs, left_children, right_children, log_score, is_finished)\n",
        "    # Khởi tạo: stack=[0], buffer=1->N, arcs=[], lc={}, rc={}, score=0.0, finished=False\n",
        "    initial_state = ([0], list(range(1, len(sentence) + 1)), [], {}, {}, 0.0, False)\n",
        "    beam = [initial_state]\n",
        "\n",
        "    # Số bước tối đa trong transition-based parsing (Arc-Standard/Arc-Eager) thường là 2*n\n",
        "    max_steps = 2 * len(sentence)\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        candidates = []\n",
        "        active_indices = []\n",
        "        \n",
        "        # --- 1. BATCHING: Collect features for all active candidates ---\n",
        "        batch_inputs = {\n",
        "            's_idx': [], 'slc_idx': [], 'src_idx': [],\n",
        "            's_pos': [], 's_lc_dep': [], 's_lc_pos': [], 's_rc_dep': [], 's_rc_pos': [],\n",
        "            'b_idx': [], 'b_pos': []\n",
        "        }\n",
        "\n",
        "        for i, state in enumerate(beam):\n",
        "            stack, buffer, arcs, lc, rc, score, finished = state\n",
        "            # Nếu trạng thái đã kết thúc hoặc không thể thực hiện thêm hành động\n",
        "            if finished or (not buffer and len(stack) <= 1):\n",
        "                candidates.append(state)\n",
        "                continue\n",
        "            \n",
        "            active_indices.append(i)\n",
        "            \n",
        "            # --- 1. TRÍCH XUẤT ĐẶC TRƯNG TỪ TRẠNG THÁI HIỆN TẠI ---\n",
        "            feat = oracle.extract_features(stack, buffer, sentence, sent_idx, lc, rc)\n",
        "            \n",
        "            # Collect features for batching\n",
        "            s_f, b_f = feat['stack'], feat['buffer']\n",
        "            batch_inputs['s_idx'].append(s_f['words'])\n",
        "            batch_inputs['slc_idx'].append(s_f['l_child_idx'])\n",
        "            batch_inputs['src_idx'].append(s_f['r_child_idx'])\n",
        "            batch_inputs['s_pos'].append(s_f['pos'])\n",
        "            batch_inputs['s_lc_dep'].append(s_f['l_child_dep'])\n",
        "            batch_inputs['s_rc_dep'].append(s_f['r_child_dep'])\n",
        "            batch_inputs['s_lc_pos'].append(s_f['l_child_pos'])\n",
        "            batch_inputs['s_rc_pos'].append(s_f['r_child_pos'])\n",
        "            batch_inputs['b_idx'].append(b_f['words'])\n",
        "            batch_inputs['b_pos'].append(b_f['pos'])\n",
        "\n",
        "        if not active_indices:\n",
        "            break\n",
        "\n",
        "        # --- 2. PREPARE BATCH TENSORS ---\n",
        "        def to_tensor(data):\n",
        "            return torch.tensor(data, dtype=torch.long, device=device)\n",
        "\n",
        "        s_idx_t = to_tensor(batch_inputs['s_idx'])\n",
        "        slc_idx_t = to_tensor(batch_inputs['slc_idx'])\n",
        "        src_idx_t = to_tensor(batch_inputs['src_idx'])\n",
        "        s_pos_t = to_tensor(batch_inputs['s_pos'])\n",
        "        s_lc_dep_t = to_tensor(batch_inputs['s_lc_dep'])\n",
        "        s_rc_dep_t = to_tensor(batch_inputs['s_rc_dep'])\n",
        "        s_lc_pos_t = to_tensor(batch_inputs['s_lc_pos'])\n",
        "        s_rc_pos_t = to_tensor(batch_inputs['s_rc_pos'])\n",
        "        b_idx_t = to_tensor(batch_inputs['b_idx'])\n",
        "        b_pos_t = to_tensor(batch_inputs['b_pos'])\n",
        "\n",
        "        # Lookup embeddings (Batch lookup)\n",
        "        s_vecs = sent_matrix[s_idx_t]\n",
        "        s_lc_vecs = sent_matrix[slc_idx_t]\n",
        "        s_rc_vecs = sent_matrix[src_idx_t]\n",
        "        b_vecs = sent_matrix[b_idx_t]\n",
        "\n",
        "        # --- 3. RUN MODEL ON BATCH ---\n",
        "        with torch.no_grad():\n",
        "            logits = model(\n",
        "                s_vecs, s_pos_t, s_lc_vecs, s_lc_dep_t, s_lc_pos_t,\n",
        "                s_rc_vecs, s_rc_dep_t, s_rc_pos_t, b_vecs, b_pos_t\n",
        "            )\n",
        "            log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "        # --- 4. GENERATE CANDIDATES ---\n",
        "        for idx_in_batch, beam_idx in enumerate(active_indices):\n",
        "            stack, buffer, arcs, lc, rc, score, finished = beam[beam_idx]\n",
        "            probs = log_probs[idx_in_batch]\n",
        "            \n",
        "            # Lấy top K hành động để giảm chi phí tính toán\n",
        "            top_k = torch.topk(probs, k=min(beam_size * 2, len(probs)))\n",
        "            \n",
        "            for i in range(top_k.values.size(0)):\n",
        "                act_id = top_k.indices[i].item()\n",
        "                log_p = top_k.values[i].item()\n",
        "                \n",
        "                action_name = id_to_action[act_id]\n",
        "                new_score = score + log_p\n",
        "                \n",
        "                # 1. Hành động SHIFT\n",
        "                if action_name == \"SHIFT\" and buffer:\n",
        "                    new_stack = stack + [buffer[0]]\n",
        "                    new_buffer = buffer[1:]\n",
        "                    candidates.append((new_stack, new_buffer, list(arcs), lc, rc, new_score, False))\n",
        "                \n",
        "                # 2. Hành động LEFT-ARC\n",
        "                elif action_name.startswith(\"LEFT-ARC\") and len(stack) >= 2:\n",
        "                    if stack[-2] != 0: # Không để ROOT làm con\n",
        "                        label = action_name.split(\"_\")[-1]\n",
        "                        dep_id = oracle.d_vocab.get(label, 0)\n",
        "                        \n",
        "                        # --- MỚI: Trích xuất POS ID của child (stack[-2]) ---\n",
        "                        child_idx = stack[-2]\n",
        "                        child_pos_str = sentence[child_idx - 1]['upos']\n",
        "                        child_pos_id = oracle.p_vocab.get(child_pos_str, 0)\n",
        "                        \n",
        "                        new_arcs = arcs + [(stack[-1], stack[-2], label)]\n",
        "                        new_stack = stack[:]\n",
        "                        new_stack.pop(-2)\n",
        "                        \n",
        "                        # Cập nhật map con trái: thêm bộ 3 (index, dep_id, pos_id)\n",
        "                        new_lc = lc.copy()\n",
        "                        head = stack[-1]\n",
        "                        new_lc[head] = new_lc.get(head, []) + [(child_idx, dep_id, child_pos_id)]\n",
        "                        \n",
        "                        candidates.append((new_stack, list(buffer), new_arcs, new_lc, rc, new_score, False))\n",
        "                \n",
        "                # 3. Hành động RIGHT-ARC\n",
        "                elif action_name.startswith(\"RIGHT-ARC\") and len(stack) >= 2:\n",
        "                    label = action_name.split(\"_\")[-1]\n",
        "                    dep_id = oracle.d_vocab.get(label, 0)\n",
        "                    \n",
        "                    # --- MỚI: Trích xuất POS ID của child (stack[-1]) ---\n",
        "                    child_idx = stack[-1]\n",
        "                    child_pos_str = sentence[child_idx - 1]['upos']\n",
        "                    child_pos_id = oracle.p_vocab.get(child_pos_str, 0)\n",
        "                    \n",
        "                    new_arcs = arcs + [(stack[-2], stack[-1], label)]\n",
        "                    new_stack = stack[:]\n",
        "                    new_stack.pop(-1)\n",
        "\n",
        "                    # Cập nhật map con phải: thêm bộ 3 (index, dep_id, pos_id)\n",
        "                    new_rc = rc.copy()\n",
        "                    head = stack[-2]\n",
        "                    new_rc[head] = new_rc.get(head, []) + [(child_idx, dep_id, child_pos_id)]\n",
        "                    \n",
        "                    candidates.append((new_stack, list(buffer), new_arcs, lc, new_rc, new_score, False))\n",
        "\n",
        "        # --- 5. PRUNING ---\n",
        "        beam = sorted(candidates, key=lambda x: x[5], reverse=True)[:beam_size]\n",
        "\n",
        "    # --- 5. LẤY KẾT QUẢ TỐT NHẤT TỪ BEAM ---\n",
        "    # State: (stack, buffer, arcs, lc, rc, score, finished)\n",
        "    best_arcs = beam[0][2]\n",
        "\n",
        "    # --- 6. TÍNH UAS/LAS ---\n",
        "    gold_heads = {t['id']: t['head'] for t in sentence}\n",
        "    gold_deps = {t['id']: t['deprel'] for t in sentence}\n",
        "    pred_info = {child: (head, label) for head, child, label in best_arcs}\n",
        "\n",
        "    correct_uas, correct_las = 0, 0\n",
        "    for child, g_head in gold_heads.items():\n",
        "        if child in pred_info:\n",
        "            p_head, p_label = pred_info[child]\n",
        "            if p_head == g_head:\n",
        "                correct_uas += 1\n",
        "                if p_label == gold_deps[child]:\n",
        "                    correct_las += 1\n",
        "\n",
        "    return correct_uas, correct_las, len(gold_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mtW282vz1vUD",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_model_parallel(model, oracle, sentences, encoded_bank, device='cpu', is_training=True):\n",
        "    model.eval()\n",
        "    model.to(device) # Chuyển model về device (cuda hoặc cpu)\n",
        "\n",
        "    id_to_action = {v: k for k, v in oracle.action_map.items()}\n",
        "    total_uas, total_las, total_deps = 0, 0, 0\n",
        "\n",
        "    action_meta = build_action_metadata(id_to_action, oracle.d_vocab)\n",
        "\n",
        "    # Sử dụng tqdm để thấy tiến trình\n",
        "    with torch.no_grad(): # Tắt gradient để tiết kiệm bộ nhớ\n",
        "        for idx, sent in enumerate(tqdm(sentences, desc=\"Evaluating\")):\n",
        "            # Gọi process_one_sentence với logic trích xuất feature mới\n",
        "            if not is_training:\n",
        "                c_uas, c_las, c_deps = process_one_sentence_beam(\n",
        "                    sent, idx, model, encoded_bank, oracle, id_to_action, device\n",
        "                )\n",
        "            else:\n",
        "                c_uas, c_las, c_deps = process_one_sentence(\n",
        "                    sent, idx, model, encoded_bank, oracle, action_meta, device\n",
        "                )\n",
        "            total_uas += c_uas\n",
        "            total_las += c_las\n",
        "            total_deps += c_deps\n",
        "\n",
        "    uas = total_uas / total_deps if total_deps > 0 else 0\n",
        "    las = total_las / total_deps if total_deps > 0 else 0\n",
        "\n",
        "    return uas, las"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def process_batch_sentences(batch_sentences, batch_indices, model, encoded_bank, oracle, action_meta, device):\n",
        "    model.eval()\n",
        "    n_sentences = len(batch_sentences)\n",
        "    \n",
        "    states = []\n",
        "    for sent in batch_sentences:\n",
        "        states.append({\n",
        "            'stack': [0],\n",
        "            'buffer': list(range(1, len(sent) + 1)),\n",
        "            'arcs': [],\n",
        "            'lc': {},\n",
        "            'rc': {},\n",
        "            'finished': False\n",
        "        })\n",
        "\n",
        "    max_steps = max(len(s) for s in batch_sentences) * 2\n",
        "    \n",
        "    for _ in range(max_steps):\n",
        "        active_indices = [i for i, s in enumerate(states) if not s['finished']]\n",
        "        if not active_indices:\n",
        "            break\n",
        "            \n",
        "        # 1. Thu thập Tensors đã trích xuất (có kích thước cố định theo window size)\n",
        "        list_s_vecs, list_slc_vecs, list_src_vecs, list_b_vecs = [], [], [], []\n",
        "        list_s_pos, list_slc_dep, list_slc_pos, list_src_dep, list_src_pos = [], [], [], [], []\n",
        "        list_b_pos = []\n",
        "        \n",
        "        for i in active_indices:\n",
        "            st = states[i]\n",
        "            sent_idx = batch_indices[i]\n",
        "            # Trích xuất đặc trưng thô (indices)\n",
        "            f = oracle.extract_features(st['stack'], st['buffer'], batch_sentences[i], \n",
        "                                        sent_idx, st['lc'], st['rc'])\n",
        "            \n",
        "            # Lấy ma trận embedding của câu từ bank\n",
        "            sent_mat = encoded_bank[sent_idx].to(device)\n",
        "            \n",
        "            # --- TRÍCH XUẤT VECTOR CỐ ĐỊNH TẠI ĐÂY ---\n",
        "            # f['stack']['words'] luôn có kích thước cố định (mặc định là 3)\n",
        "            s_idx_t = torch.tensor(f['stack']['words'], dtype=torch.long, device=device)\n",
        "            slc_idx_t = torch.tensor(f['stack']['l_child_idx'], dtype=torch.long, device=device)\n",
        "            src_idx_t = torch.tensor(f['stack']['r_child_idx'], dtype=torch.long, device=device)\n",
        "            b_idx_t = torch.tensor(f['buffer']['words'], dtype=torch.long, device=device)\n",
        "\n",
        "            list_s_vecs.append(sent_mat[s_idx_t])\n",
        "            list_slc_vecs.append(sent_mat[slc_idx_t])\n",
        "            list_src_vecs.append(sent_mat[src_idx_t])\n",
        "            list_b_vecs.append(sent_mat[b_idx_t])\n",
        "\n",
        "            # Thu thập các nhãn POS/Dep\n",
        "            def to_long_t(data): return torch.tensor(data, dtype=torch.long, device=device)\n",
        "            list_s_pos.append(to_long_t(f['stack']['pos']))\n",
        "            list_slc_dep.append(to_long_t(f['stack']['l_child_dep']))\n",
        "            list_slc_pos.append(to_long_t(f['stack']['l_child_pos']))\n",
        "            list_src_dep.append(to_long_t(f['stack']['r_child_dep']))\n",
        "            list_src_pos.append(to_long_t(f['stack']['r_child_pos']))\n",
        "            list_b_pos.append(to_long_t(f['buffer']['pos']))\n",
        "\n",
        "        # 2. Bây giờ có thể stack vì mọi tensor trong list đều cùng kích thước window\n",
        "        s_vecs = torch.stack(list_s_vecs)\n",
        "        s_lc_vecs = torch.stack(list_slc_vecs)\n",
        "        s_rc_vecs = torch.stack(list_src_vecs)\n",
        "        b_vecs = torch.stack(list_b_vecs)\n",
        "        \n",
        "        s_pos = torch.stack(list_s_pos)\n",
        "        s_lc_dep = torch.stack(list_slc_dep)\n",
        "        s_lc_pos = torch.stack(list_slc_pos)\n",
        "        s_src_dep = torch.stack(list_src_dep)\n",
        "        s_src_pos = torch.stack(list_src_pos)\n",
        "        b_pos = torch.stack(list_b_pos)\n",
        "\n",
        "        # 3. Batch Forward\n",
        "        with torch.no_grad():\n",
        "            logits = model(s_vecs, s_pos, s_lc_vecs, s_lc_dep, s_lc_pos, \n",
        "                           s_rc_vecs, s_src_dep, s_src_pos, b_vecs, b_pos)\n",
        "            top_actions = torch.argsort(logits, dim=1, descending=True)\n",
        "\n",
        "        # 4. Cập nhật trạng thái từng câu (Giữ nguyên logic transition)\n",
        "        for idx_in_active, i in enumerate(active_indices):\n",
        "            st = states[i]\n",
        "            sentence = batch_sentences[i]\n",
        "            applied = False\n",
        "            \n",
        "            for act_id in top_actions[idx_in_active].tolist():\n",
        "                meta = action_meta[act_id]\n",
        "                # Logic xử lý SHIFT, LEFT-ARC, RIGHT-ARC tương tự code cũ của bạn\n",
        "                # ... (giữ nguyên phần áp dụng hành động) ...\n",
        "                if meta['type'] == 'S' and st['buffer']:\n",
        "                    st['stack'].append(st['buffer'].pop(0)); applied = True; break\n",
        "                elif meta['type'] == 'L' and len(st['stack']) >= 2 and st['stack'][-2] != 0:\n",
        "                    h, c = st['stack'][-1], st['stack'][-2]\n",
        "                    st['arcs'].append((h, c, meta['label']))\n",
        "                    if h not in st['lc']: st['lc'][h] = []\n",
        "                    st['lc'][h].append((c, meta['dep_id'], sentence[c-1]['upos_id']))\n",
        "                    st['stack'].pop(-2); applied = True; break\n",
        "                elif meta['type'] == 'R' and len(st['stack']) >= 2:\n",
        "                    h, c = st['stack'][-2], st['stack'][-1]\n",
        "                    st['arcs'].append((h, c, meta['label']))\n",
        "                    if h not in st['rc']: st['rc'][h] = []\n",
        "                    st['rc'][h].append((c, meta['dep_id'], sentence[c-1]['upos_id']))\n",
        "                    st['stack'].pop(-1); applied = True; break\n",
        "            \n",
        "            if not applied or (not st['buffer'] and len(st['stack']) <= 1):\n",
        "                st['finished'] = True\n",
        "\n",
        "    # 5. Tính toán kết quả UAS/LAS cuối batch\n",
        "    b_uas, b_las, b_deps = 0, 0, 0\n",
        "    for i in range(n_sentences):\n",
        "        gold_heads = {t['id']: t['head'] for t in batch_sentences[i]}\n",
        "        gold_deps = {t['id']: t['deprel'] for t in batch_sentences[i]}\n",
        "        pred_info = {c: (h, l) for h, c, l in states[i]['arcs']}\n",
        "        b_uas += sum(1 for c, gh in gold_heads.items() if c in pred_info and pred_info[c][0] == gh)\n",
        "        b_las += sum(1 for c, gh in gold_heads.items() if c in pred_info and pred_info[c][0] == gh and pred_info[c][1] == gold_deps[c])\n",
        "        b_deps += len(gold_heads)\n",
        "        \n",
        "    return b_uas, b_las, b_deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_model_batch(model, oracle, sentences, encoded_bank, batch_size=64, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    id_to_action = {v: k for k, v in oracle.action_map.items()}\n",
        "    action_meta = build_action_metadata(id_to_action, oracle.d_vocab)\n",
        "    \n",
        "    total_uas, total_las, total_deps = 0, 0, 0\n",
        "    \n",
        "    # Chia sentences thành các batch\n",
        "    for i in tqdm(range(0, len(sentences), batch_size), desc=\"Batch Evaluating\"):\n",
        "        batch_sents = sentences[i : i + batch_size]\n",
        "        batch_idxs = list(range(i, i + len(batch_sents)))\n",
        "        \n",
        "        c_uas, c_las, c_deps = process_batch_sentences(\n",
        "            batch_sents, batch_idxs, model, encoded_bank, oracle, action_meta, device\n",
        "        )\n",
        "        \n",
        "        total_uas += c_uas\n",
        "        total_las += c_las\n",
        "        total_deps += c_deps\n",
        "\n",
        "    uas = total_uas / total_deps if total_deps > 0 else 0\n",
        "    las = total_las / total_deps if total_deps > 0 else 0\n",
        "    return uas, las"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77CV6CLouVOB",
        "outputId": "366183ae-6f28-46c1-8fa7-01be2ad24e16",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang đọc dữ liệu và xây dựng Vocab...\n"
          ]
        }
      ],
      "source": [
        "# 1. Khai báo đường dẫn và thiết bị\n",
        "train_path = \"UD_Vietnamese-VTB/vi_vtb-ud-train.conllu\"\n",
        "dev_path = \"UD_Vietnamese-VTB/vi_vtb-ud-dev.conllu\"\n",
        "test_path = \"UD_Vietnamese-VTB/vi_vtb-ud-test.conllu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Đang đọc dữ liệu và xây dựng Vocab...\")\n",
        "# Lưu ý: Thêm word_vocab để quản lý index cho các node con nếu cần\n",
        "# Trong cell thực hiện load dữ liệu (tương tự Cell 16 trong notebook)\n",
        "train_sentences, word_vocab, p_vocab, d_vocab = load_data_and_build_vocab([train_path], is_train=True)\n",
        "\n",
        "# GỌI HÀM NÀY NGAY TẠI ĐÂY\n",
        "train_sentences = map_sentence_to_ids(train_sentences, p_vocab, d_vocab)\n",
        "dev_sentences = map_sentence_to_ids(load_data_and_build_vocab([dev_path], is_train=False), p_vocab, d_vocab)\n",
        "test_sentences = map_sentence_to_ids(load_data_and_build_vocab([test_path], is_train=False), p_vocab, d_vocab)\n",
        "\n",
        "vocabs = (p_vocab, d_vocab)\n",
        "\n",
        "# 3. Khởi tạo Oracle\n",
        "# Oracle cần chứa hàm create_training_data_multiview sử dụng hàm extract_features mới của chúng ta\n",
        "oracle = Oracle(p_vocab, d_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang tạo chuỗi transitions cho 1400 câu...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b299b7af7b6c4429b10237e7a38a3081",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1400 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Đang trích xuất PhoBERT Embedding Bank ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c483e279aec1482ea9c47120c5e1daf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1400 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b72d0637ebc84229bb45c984eccdfcce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1123 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b4f702af0f84d95ba795f2e3bc73ce3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4. Tạo dữ liệu huấn luyện\n",
        "print(f\"Đang tạo chuỗi transitions cho {len(train_sentences)} câu...\")\n",
        "training_samples = []\n",
        "for idx, sentence in enumerate(tqdm(train_sentences)):\n",
        "    # Hàm này sẽ giả lập các bước đi (Shift, Arc) và gọi extract_features tại mỗi bước\n",
        "    examples = oracle.create_training_data_multiview(sentence, idx)\n",
        "    training_samples.extend(examples)\n",
        "\n",
        "# 6. Xây dựng ngân hàng Embedding PhoBERT (Mở comment)\n",
        "# Bank này lưu ma trận [sent_len + 1, 768] cho từng câu\n",
        "print(\"--- Đang trích xuất PhoBERT Embedding Bank ---\")\n",
        "train_encoded_bank = pre_encode_sentences(train_sentences, device=device)\n",
        "dev_encoded_bank = pre_encode_sentences(dev_sentences, device=device)\n",
        "test_encoded_bank = pre_encode_sentences(test_sentences, device=device)\n",
        "\n",
        "# 7. Cấu hình DataLoader (Mở comment và cập nhật)\n",
        "train_dataset = TransitionDataset(training_samples)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=256, # Giảm nhẹ batch size nếu VRAM bị tràn do feature quá nhiều\n",
        "    shuffle=True,\n",
        "    collate_fn=multiview_collate_fn # Phải là hàm trả về 11 thành phần\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48561\n"
          ]
        }
      ],
      "source": [
        "print(len(training_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model đã sẵn sàng với 159 hành động.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaedd4488a1648d4b647ce57ff73de1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61bddf9177e9412d91eee71bb95ec61d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Kết quả: UAS: 68.35% | LAS: 51.96%\n",
            "✅ New best model saved: best_model_bi_linear_scorer_1_0.6835.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32bf43c84b1745c19428468e5051f8c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7eb68dbf3b4dca81d18163de915fd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Kết quả: UAS: 73.02% | LAS: 58.55%\n",
            "✅ New best model saved: best_model_bi_linear_scorer_2_0.7302.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dbfc1b31241414191efd374d02849ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 3...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded6f531e2fd4c0489b2ded953c0291e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Kết quả: UAS: 74.45% | LAS: 60.15%\n",
            "✅ New best model saved: best_model_bi_linear_scorer_3_0.7445.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6d3ee4b78064bc98c92e0e3be330679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 4...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c45bc555f40947dd94b0369a698ba5de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Kết quả: UAS: 75.37% | LAS: 61.18%\n",
            "✅ New best model saved: best_model_bi_linear_scorer_4_0.7537.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5070b43ac8f34fef8495afabce367d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 5...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de81cc67cb844aa4ab73dc1d2b4e11c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ Kết quả: UAS: 74.33% | LAS: 60.21%\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c013d807811411cb4c49b26c6591e84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6/50:   0%|          | 0/190 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Đang đánh giá Epoch 6...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df14d951459a4e50ab6b3cdf2f000ea4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 8. Khởi tạo Mô hình (BỔ SUNG)\n",
        "num_actions = len(oracle.action_map)\n",
        "model = BiaffineDependencyModel(\n",
        "    pos_vocab_size=len(p_vocab),\n",
        "    dep_vocab_size=len(d_vocab),\n",
        "    num_actions=num_actions,\n",
        "    phobert_dim=768,\n",
        "    pos_dim=64,\n",
        "    dep_dim=64,\n",
        "    node_dim=256,\n",
        "    hidden_dim=1024,\n",
        "    dropout_rate=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"✅ Model đã sẵn sàng với {num_actions} hành động.\")\n",
        "\n",
        "# 9. Bắt đầu huấn luyện\n",
        "model = train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    train_encoded_bank=train_encoded_bank,\n",
        "    dev_sentences=dev_sentences,\n",
        "    dev_encoded_bank=dev_encoded_bank,\n",
        "    oracle=oracle,\n",
        "    epochs=50,\n",
        "    lr=3e-4,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# 3. Đánh giá cuối cùng trên tập TEST\n",
        "print(\"\\n--- Đánh giá cuối cùng trên tập TEST ---\")\n",
        "# Tải lại trọng số tốt nhất trước khi test\n",
        "uas, las = evaluate_model_parallel(\n",
        "    model, oracle, test_sentences, test_encoded_bank, device=device, is_training=False\n",
        ")\n",
        "print(f\"Final Result: UAS: {uas*100:.2f}% | LAS: {las*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Đã load weights thành công!\n"
          ]
        }
      ],
      "source": [
        "model_save = BiaffineDependencyModel(\n",
        "    pos_vocab_size=len(p_vocab),\n",
        "    dep_vocab_size=len(d_vocab),\n",
        "    num_actions=num_actions,\n",
        "    phobert_dim=768,\n",
        "    pos_dim=64,\n",
        "    dep_dim=64,\n",
        "    node_dim=256,\n",
        "    hidden_dim=1024,\n",
        "    dropout_rate=0.3\n",
        ").to(device)\n",
        "\n",
        "# 3. Nạp \"trọng số\" (weights) vào khung đã tạo\n",
        "weights_path = 'best_model_biaffine/best_model_bi_linear_scorer_38_0.7754.pth'\n",
        "state_dict = torch.load(weights_path, map_location=device)\n",
        "\n",
        "# Nếu file .pth của bạn chứa trực tiếp state_dict:\n",
        "model_save.load_state_dict(state_dict)\n",
        "\n",
        "# 4. Chuyển sang chế độ đánh giá\n",
        "model_save.eval()\n",
        "\n",
        "print(\"✅ Đã load weights thành công!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b18aaf5d0a4fa0bc4eec913a5bd02a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Result: UAS: 80.68% | LAS: 68.92%\n"
          ]
        }
      ],
      "source": [
        "uas, las = evaluate_model_parallel(\n",
        "    model_save, oracle, test_sentences, test_encoded_bank, device=device, is_training=False\n",
        ")\n",
        "print(f\"Final Result: UAS: {uas*100:.2f}% | LAS: {las*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
